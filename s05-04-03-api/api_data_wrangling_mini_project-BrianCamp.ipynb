{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitted by Brian Camp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise will require you to pull some data from the Qunadl API. Qaundl is currently the most widely used aggregator of financial market data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, you will need to register a free account on the http://www.quandl.com website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you register, you will be provided with a unique API key, that you should store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the API key as a string - according to PEP8, constants are always named in all upper case\n",
    "\n",
    "# to load API_KEY from another file\n",
    "# later the file 'my_api.py' will only be kept locally and not ported to github\n",
    "\n",
    "%run my_api.py # to load API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "?api_key=...\n"
     ]
    }
   ],
   "source": [
    "print(type(API_KEY))\n",
    "print(API_KEY[:9]+'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qaundl has a large number of data sources, but, unfortunately, most of them require a Premium subscription. Still, there are also a good number of free datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this mini project, we will focus on equities data from the Frankfurt Stock Exhange (FSE), which is available for free. We'll try and analyze the stock prices of a company called Carl Zeiss Meditec, which manufactures tools for eye examinations, as well as medical lasers for laser eye surgery: https://www.zeiss.com/meditec/int/home.html. The company is listed under the stock ticker AFX_X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the detailed Quandl API instructions here: https://docs.quandl.com/docs/time-series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is a dedicated Python package for connecting to the Quandl API, we would prefer that you use the *requests* package, which can be easily downloaded using *pip* or *conda*. You can find the documentation for the package here: http://docs.python-requests.org/en/master/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, apart from the *requests* package, you are encouraged to not use any third party Python packages, such as *pandas*, and instead focus on what's available in the Python Standard Library (the *collections* module might come in handy: https://pymotw.com/3/collections/ ).\n",
    "Also, since you won't have access to DataFrames, you are encouraged to us Python's native data structures - preferably dictionaries, though some questions can also be answered using lists.\n",
    "You can read more on these data structures here: https://docs.python.org/3/tutorial/datastructures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the JSON responses you will be getting from the API map almost one-to-one to Python's dictionaries. Unfortunately, they can be very nested, so make sure you read up on indexing dictionaries in the documentation provided above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #0. Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import relevant modules\n",
    "\n",
    "import requests\n",
    "from collections import Counter, OrderedDict, namedtuple\n",
    "\n",
    "from itertools import filterfalse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse\n",
    "# into the JSON structure that will be returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Quandl API documentation site at https://docs.quandl.com/docs/in-depth-usage under 'time-series data':\n",
    "\n",
    "The API involves a GET command which we will handle with the Python `requests` library\n",
    "\n",
    "`GET https://www.quandl.com/api/v3/datasets/{database_code}/{dataset_code}/data.{return_format}`\n",
    "\n",
    "In `requests` this will be implemented as\n",
    "<code>data_raw = requests.get(url)</code> and the data is then extracted as a json format using the `.json()` method.\n",
    "\n",
    "i.e. <code>data_json = data_raw.json()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2019-05-01?end_date=2019-05-07?order=asc?api_key=...\n"
     ]
    }
   ],
   "source": [
    "quandl_afxx = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json'\n",
    "\n",
    "# desired window of dates \n",
    "# for investigation of the structure, the window is kept small\n",
    "startdate_small='?start_date=2019-05-01'\n",
    "enddate_small='?end_date=2019-05-07'\n",
    "\n",
    "# constructing the string for calling the Quandl API\n",
    "#     (recall that `API_KEY` was obtained from an external program\n",
    "#      only a portion of it is printed out here)\n",
    "api_url_small = quandl_afxx + startdate_small + enddate_small + '?order=asc'\n",
    "\n",
    "print(api_url_small + API_KEY[:9] + '...')\n",
    "\n",
    "# using reqests.get() to get the raw data\n",
    "afxx_small_raw = requests.get(api_url_small+API_KEY)\n",
    "\n",
    "# extracting the data in json format:\n",
    "afxx_small_json = afxx_small_raw.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect the JSON structure of the object you created, and take note of how nested it is,\n",
    "# as well as the overall structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['dataset_data'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(afxx_small_json))\n",
    "afxx_small_json.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The json created is of type `dict`. But it only has one key: `dataset_data`. We dig inside by looking at the value that goes with the `dataset_data` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['limit', 'transform', 'column_index', 'column_names', 'start_date', 'end_date', 'frequency', 'data', 'collapse', 'order'])\n"
     ]
    }
   ],
   "source": [
    "afxx_small_dict = afxx_small_json['dataset_data']\n",
    "\n",
    "print(type(afxx_small_dict))\n",
    "\n",
    "print(afxx_small_dict.keys())\n",
    "\n",
    "#afxx_small_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside is another dict structure. This one has several keys as indicated above. Here is a closer look at the keys and values printed out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limit :  None\n",
      "transform :  None\n",
      "column_index :  None\n",
      "column_names :  ['Date', 'Open', 'High', 'Low', 'Close', 'Change', 'Traded Volume', 'Turnover', 'Last Price of the Day', 'Daily Traded Units', 'Daily Turnover']\n",
      "start_date :  2019-05-01\n",
      "end_date :  2019-05-09\n",
      "frequency :  daily\n",
      "data :  [['2019-05-09', None, 87.15, 85.25, 85.75, None, 156083.0, 13415858.0, None, None, None], ['2019-05-08', None, 88.95, 85.45, 87.0, None, 252666.0, 22022719.0, None, None, None], ['2019-05-07', None, 90.6, 87.6, 87.8, None, 306539.0, 27240006.0, None, None, None], ['2019-05-06', None, 89.05, 84.9, 89.05, None, 160083.0, 14049181.0, None, None, None], ['2019-05-03', None, 87.5, 86.65, 87.2, None, 138284.0, 12036856.0, None, None, None], ['2019-05-02', None, 87.7, 85.45, 86.55, None, 216631.0, 18702265.0, None, None, None]]\n",
      "collapse :  None\n",
      "order :  None\n"
     ]
    }
   ],
   "source": [
    "for key, values in afxx_small_dict.items():\n",
    "    print(key,': ',values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the relevant keys are `column_names` and `data`. The `column_names` are the names of the entries in the `data` column.\n",
    "\n",
    "Let's look a bit more closely at these two keys and their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "['Date', 'Open', 'High', 'Low', 'Close', 'Change', 'Traded Volume', 'Turnover', 'Last Price of the Day', 'Daily Traded Units', 'Daily Turnover']\n",
      "11\n",
      "['2019-05-09', None, 87.15, 85.25, 85.75, None, 156083.0, 13415858.0, None, None, None]\n",
      "['2019-05-08', None, 88.95, 85.45, 87.0, None, 252666.0, 22022719.0, None, None, None]\n",
      "['2019-05-07', None, 90.6, 87.6, 87.8, None, 306539.0, 27240006.0, None, None, None]\n",
      "['2019-05-06', None, 89.05, 84.9, 89.05, None, 160083.0, 14049181.0, None, None, None]\n",
      "['2019-05-03', None, 87.5, 86.65, 87.2, None, 138284.0, 12036856.0, None, None, None]\n",
      "['2019-05-02', None, 87.7, 85.45, 86.55, None, 216631.0, 18702265.0, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "print(len(afxx_small_dict['column_names']))\n",
    "print(afxx_small_dict['column_names'])\n",
    "print(len(afxx_small_dict['data'][0]))\n",
    "\n",
    "for each in afxx_small_dict['data']:\n",
    "    print(each)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few other issues that seem to be present.\n",
    "* The dates are given in reverse order even though the original API call asked for ascending order with `?order=asc`.<br>\n",
    "* The end date of 2017-05-07 was not respected.  In other words, the API call ignored the `?enddate=2019-05-07` part of the query.<br>\n",
    "* Some of the data in the `Open` field is `None` which will pose problems for some questions later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with the data further, a `namedtuple` called `MyRow` will be created. This will allow the data on any given day to still\n",
    "be associated with the column name that it is from. Some of the column names have spaces however, so to these will be changed to underscores. The `MyRow` tuple will not include the date as that will be used as an index of sorts for a list that we create later. Keeping the date out of the `namedtuple` will also make it a bit easier to filter data later on (since there are some dates showing up that we don't wont -- more below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Open', 'High', 'Low', 'Close', 'Change', 'Traded_Volume', 'Turnover', 'Last_Price_of_the_Day', 'Daily_Traded_Units', 'Daily_Turnover']\n"
     ]
    }
   ],
   "source": [
    "tuple_names = [i.replace(' ','_') for i in afxx_small_dict['column_names'][1:]]\n",
    "print(tuple_names)\n",
    "MyRow = namedtuple('MyRow', tuple_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function `datalist(data)` that will be used to put our data into a list of tuples. The tuples will be of the form `( date, OrderedDict() )` where the `MyRow()._asdict()` will build Ordered Dictionaries to contain the data for a given date. Finally, the list will be `sorted()` by the `date` entry in the tuple.\n",
    "\n",
    "Note: both `OrderedDict()` and `namedtuple` are from the Python `collections` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_list(data):\n",
    "    # list of tuples\n",
    "    # first entry is date\n",
    "    # second entry is namedtuple MyRow that contains the data from that date.\n",
    "    \n",
    "    l = sorted([(each[0], MyRow(*each[1:])._asdict()) for each in data])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example to create the list of data\n",
    "# data_list(afxx_small_dict['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are your tasks for this mini project:\n",
    "\n",
    "1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD).\n",
    "2. Convert the returned JSON object into a Python dictionary.\n",
    "3. Calculate what the highest and lowest opening prices were for the stock in this period.\n",
    "4. What was the largest change in any one day (based on High and Low price)?\n",
    "5. What was the largest change between any two days (based on Closing Price)?\n",
    "6. What was the average daily trading volume during this year?\n",
    "7. (Optional) What was the median trading volume during this year. (Note: you may need to implement your own function for calculating the median.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1. Collect data for 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01?end_date=2017-12-31?order=asc?api_key=...\n"
     ]
    }
   ],
   "source": [
    "quandl_afxx = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json'\n",
    "\n",
    "startdate='?start_date=2017-01-01'\n",
    "enddate='?end_date=2017-12-31'\n",
    "\n",
    "api_url = quandl_afxx + startdate + enddate + '?order=asc' + API_KEY\n",
    "#print(api_url)\n",
    "print(quandl_afxx + startdate + enddate + '?order=asc' + API_KEY[:9] + '...')\n",
    "\n",
    "afxx_raw = requests.get(api_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2. convert to json and  then to dict format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.json()` method will extract the data in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "afxx_json = afxx_raw.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw above, we need to extract the dictionary that is paired with `dataset` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "afxx_dict = afxx_json['dataset']\n",
    "#afxx_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we use our function `data_list()` which was defined above on the `data` values in the `afxx_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "afxx = data_list(afxx_dict['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "('2017-01-02', OrderedDict([('Open', 34.99), ('High', 35.94), ('Low', 34.99), ('Close', 35.8), ('Change', None), ('Traded_Volume', 44700.0), ('Turnover', 1590561.0), ('Last_Price_of_the_Day', None), ('Daily_Traded_Units', None), ('Daily_Turnover', None)]))\n"
     ]
    }
   ],
   "source": [
    "print(type(afxx))\n",
    "print(afxx[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some problems with this data as we noted above. The first problem is that we have too much data. The goal was to have data that occurred between `2017-01-01` and `2017-12-31`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585\n",
      "('2019-05-09', OrderedDict([('Open', None), ('High', 87.15), ('Low', 85.25), ('Close', 85.75), ('Change', None), ('Traded_Volume', 156083.0), ('Turnover', 13415858.0), ('Last_Price_of_the_Day', None), ('Daily_Traded_Units', None), ('Daily_Turnover', None)]))\n"
     ]
    }
   ],
   "source": [
    "# many entries are from outside the time period\n",
    "# for example, the last entry is outside of the time period of interest\n",
    "print(len(afxx))\n",
    "print(afxx[-1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove unwanted data we will make use of the `filterfalse()` function from the Python `itertools` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "afxx = list(filterfalse(lambda x: x[0]>'2017-12-31', afxx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the last piece of data is within the time period and the length of dataset corresponds to roughly fifty weeks of five business days apiece during the year (which is normal for financial markets in a year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "2017-12-29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(afxx))\n",
    "print(afxx[-1][0])\n",
    "afxx[-1][0]<'2018-01-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the remaining questions we need extract several lists of data from our dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [ x[0] for x in afxx ]\n",
    "opening = [ x[1]['Open'] for x in afxx ]\n",
    "highs = [ x[1]['High'] for x in afxx ]\n",
    "lows = [ x[1]['Low'] for x in afxx ]\n",
    "closing = [ x[1]['Close'] for x in afxx ]\n",
    "traded_vol = [ x[1]['Traded_Volume'] for x in afxx ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3. Highest and Lowest Opening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we encounter another problem. Not all of the opening data has a number. Some occur as `None`.\n",
    "\n",
    "We can use the `Counter()` function from the Python `collections` module along with its method `.most_common()` to see how many different types of data we have for the opening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(float, 252), (NoneType, 3)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#open_type = [type(each) for each in opening]\n",
    "Counter([type(each) for each in opening]).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with stock data, we will replace any missing opening data by using the closing price from the previous day.\n",
    "\n",
    "Note: The first piece of data does not have this issue. Otherwise we would need to find data from the previous year to fill in the opening price (i.e. the last closing price from 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.99\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(opening[0])\n",
    "print(type(opening[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a new list `newopen` to repair the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "newopen = []\n",
    "for i, val in enumerate(opening):\n",
    "    # if None then use previous closing value\n",
    "    if val == None:        \n",
    "        newopen.append(closing[i-1])\n",
    "    else:\n",
    "        newopen.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can more easily find the minimum and maximum opening values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest opening value is 34.0. Occured on: ['2017-01-24']\n",
      "Highest opening value is 53.11. Occured on: ['2017-12-14']\n"
     ]
    }
   ],
   "source": [
    "lowval = min(newopen)\n",
    "highval = max(newopen)\n",
    "lowest = {'val':lowval, 'ind':[index for index, value in enumerate(newopen) if value==lowval]}\n",
    "highest = {'val':highval, 'ind':[index for index, value in enumerate(newopen) if value==highval]}\n",
    "\n",
    "\n",
    "print('Lowest opening value is %s. Occured on: %s'%(lowest['val'], [dates[i] for i in lowest['ind']]))\n",
    "print('Highest opening value is %s. Occured on: %s'%(highest['val'], [dates[i] for i in highest['ind']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #4. Largest range within a given day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest range within a given day will be based upon comparing the high and low prices on any given day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest range on any given day (from low to high) was 2.8100000000000023.\n",
      "Occured on: ['2017-05-11']\n"
     ]
    }
   ],
   "source": [
    "daily_changes = [ highs[i] - lows[i] for i in range(len(dates)) ]\n",
    "big_change_val = max(daily_changes)\n",
    "biggest_change = {'val':big_change_val, 'ind':[index for index, value in enumerate(daily_changes) if value==big_change_val]}\n",
    "biggest_change\n",
    "print('Biggest range on any given day (from low to high) was %s.\\nOccured on: %s'%(biggest_change['val'], [dates[i] for i in biggest_change['ind']]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #5. Largest change between any two days (based on closing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no missing data from the `closing` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(float, 255)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([type(each) for each in closing]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_consecutive = [ closing[i+1] - closing[i] for i in range(len(closing)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to consider both largest negative (decrease) or largest positive (increase) when finding the biggest change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest increase was 1.7199999999999989\n",
      "Biggest decrease was -2.559999999999995\n",
      "Biggest change (by absolute value) was 2.559999999999995\n"
     ]
    }
   ],
   "source": [
    "big_inc = max(change_consecutive)\n",
    "big_dec = min(change_consecutive)\n",
    "\n",
    "print('Biggest increase was %s'%big_inc)\n",
    "print('Biggest decrease was %s'%big_dec)\n",
    "\n",
    "big_change = max([abs(big_inc), abs(big_dec)])\n",
    "\n",
    "print('Biggest change (by absolute value) was %s'%big_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #6. Average Daily Trading Volume during 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average daily trading volume during 2017 was 89124.33725490196\n"
     ]
    }
   ],
   "source": [
    "ave_daily_traded = sum(traded_vol)/len(traded_vol)\n",
    "\n",
    "print('The average daily trading volume during 2017 was %s'%ave_daily_traded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #7. Median Trading Volume during 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function `my_median(data)` to calculate the median of a list of **sorted** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_median(data):\n",
    "    n = len(data)\n",
    "    sortdata = sorted(data)\n",
    "    # if even number\n",
    "    if n%2 == 0:\n",
    "        med = (sortdata[int(n/2) - 1] + sortdata[int(n/2)])/2\n",
    "    else:\n",
    "        med = sortdata[int((n+1)/2) - 1]\n",
    "    return med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median volume traded was 76286.0\n"
     ]
    }
   ],
   "source": [
    "median_trade_vol = my_median(traded_vol)\n",
    "\n",
    "print('Median volume traded was %s'%median_trade_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
